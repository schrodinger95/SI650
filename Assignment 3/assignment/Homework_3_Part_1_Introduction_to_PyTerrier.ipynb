{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 3 Part 1 - Introduction to PyTerrier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5a8240ceb63417c84032de0f7a1b283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e1e8ba53b57469ab09eadb6058a06f8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14e424bc221b4c8d9efc27b6bc607e2b",
              "IPY_MODEL_d2e8e046af6a47f5b2437b00d418e3b1",
              "IPY_MODEL_6bcdb827cd904aa294b169734611699a"
            ]
          }
        },
        "3e1e8ba53b57469ab09eadb6058a06f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14e424bc221b4c8d9efc27b6bc607e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9554506835ea48619d628b07b45bca73",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "cord19/trec-covid documents: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef448bb09ad74fe691fe032bbf0e1867"
          }
        },
        "d2e8e046af6a47f5b2437b00d418e3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a69fd279ee3b45abbab6d88eb48efc53",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 192509,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 192509,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d6a04fcdd884a93a82af8155d331727"
          }
        },
        "6bcdb827cd904aa294b169734611699a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1f9ad2bf8f8b4b9eb28ad7acfcb9d70e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 192509/192509 [01:33&lt;00:00, 2163.82it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e5eec716ed0b433ebadc843048dfcd3e"
          }
        },
        "9554506835ea48619d628b07b45bca73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef448bb09ad74fe691fe032bbf0e1867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a69fd279ee3b45abbab6d88eb48efc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d6a04fcdd884a93a82af8155d331727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f9ad2bf8f8b4b9eb28ad7acfcb9d70e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e5eec716ed0b433ebadc843048dfcd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UxEkLc6yz6J"
      },
      "source": [
        "# SI 650 / EECS 549: Homework 3 Part 1\n",
        "## Introduction to PyTerrier \n",
        "\n",
        "This homework is intended to expose you to other types of information retrieval and demonstrates the use of another state of the art IR library, [PyTerrier](https://github.com/terrier-org/pyterrier). \n",
        "\n",
        "The overall learning goals of the assignment across all three parts are\n",
        "  - Learn how to use PyTerrier\n",
        "  - Understand how to train and use a Learning to Rank model\n",
        "  - Understand how to train and use a dense vector retrieval (using deep learning)\n",
        "  - Understand how to use document augmentation\n",
        "  - Gain additional programming and debugging skills when working with modern IR libraries\n",
        "  - Learn how to use the [Great Lakes cluster](https://arc.umich.edu/greatlakes/)\n",
        "  \n",
        "  \n",
        "The Great Lakes cluster is a collection of high performance computers at the University of Michigan. The big advantage for this course is the ability to use its GPUs for doing deep learning. You will have access to this cluster for Homework 3 _and_ for your course project, which can expand the type of methods you can try. When launching jobs for this course, be sure to have your job use the `si650f21_class` account.\n",
        "\n",
        "For this assignment, we'll be using the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge), which is a collection of documents about Covid-19 produced by AI2. In places, we've pretrained models and precomputed indices for you (which can take large amounts of time), but we'll ask you to try out the commands on a small scale so you'll know how to run them.\n",
        "\n",
        "Homework 3 Part 1 will have you working on the following tasks to get you started:\n",
        "  - PyTerrier installation & configuration\n",
        "  - indexing a collection\n",
        "  - accessing an index\n",
        "  - using the `BatchRetrieve` transformer for searching an index\n",
        "  - conducting an `Experiment` \n",
        "\n",
        "For all parts of the homework, you can run them on your local computer with enough time. However, for Part 3, you will see *significant* speed up running these as notebooks on Great Lakes with a GPU. The three parts are designed to be completed in order, as they build on each other conceptually.\n",
        "\n",
        "For each notebook, all the tasks that you will need to complete are marked with **Task** in a cell title comment.\n",
        "\n",
        "Note that just like Pyserini, PyTerrier also uses a Java-based  library underneath, [Terrier information retrieval toolkit](http://terrier.org), so you will need to set `JAVA_HOME` accordingly. underlying for many indexing and retrieval operations. PyTerrier is relatively new in 2020, but Terrier has a long history dating back to 2001 and  makes it easy to perform IR experiments in Python, which could come in handy for you when doing your course project.\n",
        "\n",
        "See the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/) for many more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u2hD-zBzfpR"
      },
      "source": [
        "PyTerrier is a Python framework, but uses the underlying [Terrier information retrieval toolkit](http://terrier.org) for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations. \n",
        "\n",
        "In the following, we introduce everything you need to know about PyTerrier, and also provide appropriate links to relevant parts of the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHR60e39Je4R"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJdI8CmMJ7yt",
        "outputId": "39e01bd6-0840-46a1-8ee9-a2971a892d58"
      },
      "source": [
        "!pip install python-terrier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-terrier\n",
            "  Downloading python-terrier-0.7.1.tar.gz (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.1.5)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from python-terrier) (4.62.3)\n",
            "Collecting pyjnius~=1.3.0\n",
            "  Downloading pyjnius-1.3.0-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.3 MB/s \n",
            "\u001b[?25hCollecting matchpy\n",
            "  Downloading matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from python-terrier) (0.0)\n",
            "Collecting deprecation\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting chest\n",
            "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from python-terrier) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from python-terrier) (1.1.0)\n",
            "Collecting nptyping\n",
            "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.7/dist-packages (from python-terrier) (8.11.0)\n",
            "Collecting ir_datasets>=0.3.2\n",
            "  Downloading ir_datasets-0.4.3-py3-none-any.whl (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from python-terrier) (2.11.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from python-terrier) (0.10.2)\n",
            "Collecting ir_measures>=0.2.0\n",
            "  Downloading ir_measures-0.2.1.tar.gz (36 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from python-terrier) (0.3.4)\n",
            "Collecting warc3-wet>=0.2.3\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting lz4>=3.1.1\n",
            "  Downloading lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 31.8 MB/s \n",
            "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "Collecting ijson>=3.1.3\n",
            "  Downloading ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 48.6 MB/s \n",
            "\u001b[?25hCollecting pyautocorpus>=0.1.1\n",
            "  Downloading pyautocorpus-0.1.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from ir_datasets>=0.3.2->python-terrier) (4.6.3)\n",
            "Collecting trec-car-tools>=2.5.4\n",
            "  Downloading trec_car_tools-2.5.4-py3-none-any.whl (8.1 kB)\n",
            "Collecting lxml>=4.5.2\n",
            "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting zlib-state>=0.1.3\n",
            "  Downloading zlib_state-0.1.3-cp37-cp37m-manylinux2010_x86_64.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting pytrec-eval-terrier==0.5.1\n",
            "  Downloading pytrec_eval_terrier-0.5.1-cp37-cp37m-manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 57.7 MB/s \n",
            "\u001b[?25hCollecting cwl-eval>=1.0.10\n",
            "  Downloading cwl-eval-1.0.10.tar.gz (31 kB)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius~=1.3.0->python-terrier) (1.15.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from pyjnius~=1.3.0->python-terrier) (0.29.24)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->python-terrier) (2.10)\n",
            "Collecting cbor>=1.0.0\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from chest->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation->python-terrier) (21.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->python-terrier) (2.0.1)\n",
            "Collecting multiset<3.0,>=2.0\n",
            "  Downloading multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
            "Collecting typish>=1.7.0\n",
            "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation->python-terrier) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->python-terrier) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->python-terrier) (2018.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->python-terrier) (3.0.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels->python-terrier) (0.5.2)\n",
            "Building wheels for collected packages: python-terrier, ir-measures, cwl-eval, cbor, warc3-wet-clueweb09, chest, wget\n",
            "  Building wheel for python-terrier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-terrier: filename=python_terrier-0.7.1-py3-none-any.whl size=102452 sha256=35c1c4a432984d492c9eaea982a33cbd954f69a0e0a95badeb15b5897a2f60ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/9a/c8/1c2d9ec6a1494bb54f47e0d2627b5ed7b2de704b66723d3417\n",
            "  Building wheel for ir-measures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ir-measures: filename=ir_measures-0.2.1-py3-none-any.whl size=46421 sha256=a3194ac97a3552411d050f19b1e6ac532e934a49809cb08fd595c6e52b3b6752\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/a4/34/d0b2e6c329f3d0fab3d3c3ed296b963cee47872811acdc3628\n",
            "  Building wheel for cwl-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cwl-eval: filename=cwl_eval-1.0.10-py3-none-any.whl size=37795 sha256=fb2b825e5749876f2803d85147f8d323b5408e24a2729cb03599cc0a503c3623\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/e9/ff/d2b6d72d9feb0d0b1b11aacfaf5cd866717034615c2d194093\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp37-cp37m-linux_x86_64.whl size=51299 sha256=e4feab993f933cbf7d48bb9d0d16fc5db1637a41b8ca48b2bb699f900883438d\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/77/49/c9c2c8dc5848502e606e8579d0bbda18b850fb056a6c62239d\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18921 sha256=7315f7abb94aa933ca17669b760dcd9031d56b0004d1d106aba0048292d31431\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/d4/3c/7c2b0c3d400ad744e4db69f2fde166655da2ed2198bfc02db6\n",
            "  Building wheel for chest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7632 sha256=a50b195151e48cbc88dd7985b16893ec447ae296d2fb2a3836f88ec51434423c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/f5/b9/c436e11300809e6b40d46a5d2592fb0bff89e0712f2e878dc7\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=b48ad173bfb117022618e2c34792ae3262dcfc4acbca2422a38468607e2e481d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built python-terrier ir-measures cwl-eval cbor warc3-wet-clueweb09 chest wget\n",
            "Installing collected packages: cbor, zlib-state, warc3-wet-clueweb09, warc3-wet, typish, trec-car-tools, pyyaml, pytrec-eval-terrier, pyautocorpus, multiset, lz4, lxml, ijson, deprecation, cwl-eval, wget, pyjnius, nptyping, matchpy, ir-measures, ir-datasets, chest, python-terrier\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed cbor-1.0.0 chest-0.2.3 cwl-eval-1.0.10 deprecation-2.1.0 ijson-3.1.4 ir-datasets-0.4.3 ir-measures-0.2.1 lxml-4.6.4 lz4-3.1.3 matchpy-0.5.5 multiset-2.1.1 nptyping-1.4.4 pyautocorpus-0.1.6 pyjnius-1.3.0 python-terrier-0.7.1 pytrec-eval-terrier-0.5.1 pyyaml-6.0 trec-car-tools-2.5.4 typish-1.9.3 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 wget-3.2 zlib-state-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS4pYphvJe4S"
      },
      "source": [
        "import pandas as pd\n",
        "# Helpful for showing indexing information\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "\n",
        "import pyterrier as pt\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH0Ds2370V0G"
      },
      "source": [
        "### Starting PyTerrier\n",
        "\n",
        "The first step is to initialize PyTerrier using PyTerrier's `init()` method. The `init()` method will download Terrier's jar file (if it's not already) and then start the Java Virtual Machine. To avoid downstream complications, we check `started()` prior to calling `init()` to prevent multiple Terrier instances from running concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4qALBa90-7g",
        "outputId": "458082fb-7802-446d-a47c-4a12369ba512"
      },
      "source": [
        "if not pt.started():\n",
        "    pt.init()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terrier-assemblies 5.6 jar-with-dependencies not found, downloading to /root/.pyterrier...\n",
            "Done\n",
            "terrier-python-helper 0.0.6 jar not found, downloading to /root/.pyterrier...\n",
            "Done\n",
            "PyTerrier 0.7.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qqjVSu-5_FX"
      },
      "source": [
        "### Documents, Indexing and Indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3soS1IIy5B83"
      },
      "source": [
        "PyTerrier typically works with Pandas dataframes for inputs. Let's create a toy set of documents in a dataframe to test. Note that the column name of `docno` is a special PyTerrier name that is the unique identifier for each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "gSEiEuTE5uyL",
        "outputId": "3e0fc702-53a7-44b6-a867-e14cc5d3d605"
      },
      "source": [
        "docs_df = pd.DataFrame([\n",
        "        [\"d1\", \"this is the first document of many documents\"],\n",
        "        [\"d2\", \"this is another document\"],\n",
        "        [\"d3\", \"the topic of this document is unknown\"]\n",
        "    ], columns=[\"docno\", \"text\"])\n",
        "\n",
        "docs_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docno</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d1</td>\n",
              "      <td>this is the first document of many documents</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d2</td>\n",
              "      <td>this is another document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d3</td>\n",
              "      <td>the topic of this document is unknown</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  docno                                          text\n",
              "0    d1  this is the first document of many documents\n",
              "1    d2                      this is another document\n",
              "2    d3         the topic of this document is unknown"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RCtCCTU6GAj"
      },
      "source": [
        "Before any search engine can estimate which documents are most likely to be relevant for a given query, it must index the documents. \n",
        "\n",
        "In the following cell, we index the dataframe's documents. The index, with all its data structures, is written into a directory called `toydocs_index`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1YvLhEOS6V8w",
        "outputId": "eb0ea676-0998-448a-fd13-4d77b4e12211"
      },
      "source": [
        "index_dir = './toydocs_index'\n",
        "indexer = pt.DFIndexer(index_dir, overwrite=True)\n",
        "index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n",
        "index_ref.toString()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./toydocs_index/data.properties'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUm6r6_625gW"
      },
      "source": [
        "PyTerrier will generate a index in the `toydocs_index` directory and and we can list the files to see what kind of internal structure and files it made"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF45pl5O8p7R",
        "outputId": "e961cd29-d02c-4330-9a3c-35525df61f9e"
      },
      "source": [
        "os.listdir(index_dir)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data.inverted.bf',\n",
              " 'data.meta.zdata',\n",
              " 'data.document.fsarrayfile',\n",
              " 'data.lexicon.fsomaphash',\n",
              " 'data.properties',\n",
              " 'data.meta-0.fsomapfile',\n",
              " 'data.meta.idx',\n",
              " 'data.direct.bf',\n",
              " 'data.lexicon.fsomapfile']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2b8isFP3Kv6"
      },
      "source": [
        "Once we've generated the files associated with `index_ref`, we can load the information into an actual PyTerrier index using the method `pt.IndexFactory.of()`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTM17szD6pNy"
      },
      "source": [
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZe3HD5i7G3v"
      },
      "source": [
        "See Terrier's [`Index`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html) object for documentation, which is written in Java's Javadoc format. We can call these methods on our index object as well. Important methods to note are:\n",
        " - `getCollectionStatistics()`\n",
        " - `getInvertedIndex()`\n",
        " - `getLexicon()`\n",
        "\n",
        "Let's see what is returned by the `CollectionStatistics()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-gXEDSX65bx",
        "outputId": "f8c569e9-be76-467a-aa8d-0100721deee0"
      },
      "source": [
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 3\n",
            "Number of terms: 4\n",
            "Number of postings: 6\n",
            "Number of fields: 0\n",
            "Number of tokens: 7\n",
            "Field names: []\n",
            "Positions:   false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6HrR4lc7i10"
      },
      "source": [
        "Let's unpack the statistics a bit more. We have 3 documents but why do we have only 4 unique terms? We can look at which terms we have by getting the [`Lexicon`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Lexicon.html) object, which contains our vocabulary. We can iterate over the `Lexicon` from Python like a dictionary to see which terms are present and what information there is about each term after indexing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us2mAzTW7Bny",
        "outputId": "c7eeb2b4-44e0-4e8c-dd2f-dbf8a5a4bfbe"
      },
      "source": [
        "for kv in index.getLexicon():\n",
        "    # Let's all print the type information of each to get a sense of what we're working with\n",
        "    print(\"%s (%s) -> %s (%s)\" % (kv.getKey(), type(kv.getKey()), kv.getValue().toString(), type(kv.getValue()) ) )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document (<class 'str'>) -> term0 Nt=3 TF=4 maxTF=2 @{0 0 0} (<class 'jnius.reflect.org.terrier.structures.LexiconEntry'>)\n",
            "first (<class 'str'>) -> term1 Nt=1 TF=1 maxTF=1 @{0 0 7} (<class 'jnius.reflect.org.terrier.structures.LexiconEntry'>)\n",
            "topic (<class 'str'>) -> term2 Nt=1 TF=1 maxTF=1 @{0 1 1} (<class 'jnius.reflect.org.terrier.structures.LexiconEntry'>)\n",
            "unknown (<class 'str'>) -> term3 Nt=1 TF=1 maxTF=1 @{0 1 5} (<class 'jnius.reflect.org.terrier.structures.LexiconEntry'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwbp94gh86pw"
      },
      "source": [
        "Iterating over the `Lexicon` shows that we're mapping a `String ` term to a [`LexiconEntry`](http://terrier.org/docs/current/javadoc/org/terrier/structures/LexiconEntry.html) object, which itself is an [`EntryStatistics`](http://terrier.org/docs/current/javadoc/org/terrier/structures/EntryStatistics.html). The `LexiconEntry` contains information including the statistics of that term.\n",
        "\n",
        "Looking at what we indexed reveals that PyTerrier is removing stopwords for us, much like Pyserini did. PyTerrier is also doing some token normalization as well so that we only have \"document\" in our index, even though document `d1` has the token \"documents\"! By default, Terrier removes standard stopwords and applies Porter's stemmer (which we talked about in class), though these behaviors can be configured.\n",
        "\n",
        "The `EntryStatistics` also provides a few other fields that offer insights:\n",
        " - `Nt` is the number of unique documents that each term occurs in – this is useful for calculating IDF.\n",
        " - `TF` is the total number of occurrences – some weighting models use this instead of Nt.\n",
        " - The numbers in the `@{}` are a pointer – they tell Terrier where the postings are for that term in the inverted index data structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMNYDYSPJe4n"
      },
      "source": [
        "PyTerrier also supports directly looking up a word using the `[]` operator, much like we would if we were looking up a key's value in a dictionary. Let's look up the value for the word \"document\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZmi9498-Ijw",
        "outputId": "7e19fc5b-a934-4d52-93bf-601e490c8841"
      },
      "source": [
        "print(index.getLexicon()[\"document\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "term0 Nt=3 TF=4 maxTF=2 @{0 0 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaKaU59l-kzg"
      },
      "source": [
        "We can use the information in the `Lexicon` to also look up documents as well. Remember from class that an inverted index is a mapping from a term to which *documents* each term occurs in. The `LexiconEntry` for a word contains the pointer to where to find the documents for that word in the inverted index. \n",
        "\n",
        "The object retrieved from using the `[]` operator with a `Lexicon` is a pointer that we can use with the inverted index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQki_Pds8ut2",
        "outputId": "5f76ef88-6614-499f-ff32-42343ba00a31"
      },
      "source": [
        "pointer = index.getLexicon()[\"document\"]\n",
        "for posting in index.getInvertedIndex().getPostings(pointer):\n",
        "    print(str(posting) + \" doclen=%d\" % posting.getDocumentLength())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID(0) TF(2) doclen=3\n",
            "ID(1) TF(1) doclen=1\n",
            "ID(2) TF(1) doclen=3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EaoIIO_DPx"
      },
      "source": [
        "From this output, we can see that the term \"document\" occurs in all three documents, as well as how long those documents are. Note that PyTerrier starts counting indexed documents with `int` values starting from 0 (called *docids*). These *docids* are then mapped back to *docnos*, which are the unique string identifiers for a document, e.g., the \"`d1`\", \"`d2`\" we used. This mapping is stored in a separate data structure called the *metaindex*, though you likely won't need to use that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOSdVAr-CGRf"
      },
      "source": [
        "## Searching an Index\n",
        "\n",
        "Our way into search in PyTerrier is called `BatchRetrieve`. BatchRetrieve is configured by specifying an index and a weighting model. Here', we'll use the `Tf` weighting, which is just term frequency; there are multiple possible weighting schemes, as we'll see later. Using a `BatchRetrieve` object, we will search for a single-word query, `\"document\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "XtK93nwXCF5C",
        "outputId": "26723c17-68b0-4759-cee0-8b0904453c49"
      },
      "source": [
        "br = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
        "br.search(\"document\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "      <th>docno</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>d1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>d2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>d3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid  docid docno  rank  score     query\n",
              "0   1      0    d1     0    2.0  document\n",
              "1   1      1    d2     1    1.0  document\n",
              "2   1      2    d3     2    1.0  document"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHqSfTCtDM2T"
      },
      "source": [
        "The `search()` method returns a Pandas dataframe with columns:\n",
        " - `qid`: this is the query id, which is by default \"1\", since we issued only one query\n",
        " - `docid`: Terrier' internal integer for each document\n",
        " - `docno`: the external (string) unique identifier for each document\n",
        " - `score`: since we use the `Tf` weighting model, this score corresponds to the total frequency of the query (terms) in each document\n",
        " - `rank`: A handy attribute showing the descending order by score\n",
        " - `query`: the input query\n",
        "\n",
        "As expected, the `Tf` weighting model used here only counts the frequencies of the query terms in each document, i.e.:\n",
        "$$\n",
        "score(d,q) = \\sum_{t \\in q} tf_{t,d}\n",
        "$$\n",
        "\n",
        "Hence, it's clear that document `d1` should be the highest scored document with two occurrences (c.f. `'document'` and `'documents'`).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBXquPOD6q7"
      },
      "source": [
        "### Searching with multiple queries\n",
        "\n",
        "We can search for more than one query at a time using the  `transform()` method rather than the `search()` method. PyTerrier uses the notion of transformers, which we'll describe much more in Part 2, but for now, you can think of this function as transforming some input to some output. In our case, we'll create a Pandas DataFrame with our queries, which we'll provide as input to the `BatchRetrieve` object, to \"transform\" into results.\n",
        "\n",
        "Note that we not only need to provide queries, but also query identifiers in the `qid` column. These `qid` values will let us distinguish which results go to which query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "LNpgELd2Je4p",
        "outputId": "b6379649-03f6-403a-bbf2-a1765a624efe"
      },
      "source": [
        "queries = pd.DataFrame([[\"q1\", \"document\"], [\"q2\", \"first document\"]], columns=[\"qid\", \"query\"])\n",
        "queries"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>q1</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>q2</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid           query\n",
              "0  q1        document\n",
              "1  q2  first document"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__GFEw8QJe4p"
      },
      "source": [
        "Now we can pass this queries data frame into `transform()` to get the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TPBmPOETBKWk",
        "outputId": "c46bca80-1358-4548-a4b4-c74e79e274bf"
      },
      "source": [
        "br.transform(queries)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "      <th>docno</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>q1</td>\n",
              "      <td>0</td>\n",
              "      <td>d1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>q1</td>\n",
              "      <td>1</td>\n",
              "      <td>d2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>q1</td>\n",
              "      <td>2</td>\n",
              "      <td>d3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>q2</td>\n",
              "      <td>0</td>\n",
              "      <td>d1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>q2</td>\n",
              "      <td>1</td>\n",
              "      <td>d2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>q2</td>\n",
              "      <td>2</td>\n",
              "      <td>d3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid  docid docno  rank  score           query\n",
              "0  q1      0    d1     0    2.0        document\n",
              "1  q1      1    d2     1    1.0        document\n",
              "2  q1      2    d3     2    1.0        document\n",
              "3  q2      0    d1     0    3.0  first document\n",
              "4  q2      1    d2     1    1.0  first document\n",
              "5  q2      2    d3     2    1.0  first document"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcgDzFLBEWAI"
      },
      "source": [
        "Most common operations in PyTerrier have to be overloaded so that you can call them using python syntax (called _operator overloading_). We'll discuss this more in Part 2, but for now, know that you can call `br.transform(queries)` using just `br(queries)`. Here. the `()` operator has been overloaded so that it calls `transform()` for us! You will see this usage very frequently in examples and documentation so it's worth noting and remembering the two are equivalent. As an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "YCwxb3HhEOp_",
        "outputId": "84b329f4-9d29-4346-a305-6df3ca9e0faa"
      },
      "source": [
        "br(queries)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "      <th>docno</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>q1</td>\n",
              "      <td>0</td>\n",
              "      <td>d1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>q1</td>\n",
              "      <td>1</td>\n",
              "      <td>d2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>q1</td>\n",
              "      <td>2</td>\n",
              "      <td>d3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>q2</td>\n",
              "      <td>0</td>\n",
              "      <td>d1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>q2</td>\n",
              "      <td>1</td>\n",
              "      <td>d2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>q2</td>\n",
              "      <td>2</td>\n",
              "      <td>d3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>first document</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid  docid docno  rank  score           query\n",
              "0  q1      0    d1     0    2.0        document\n",
              "1  q1      1    d2     1    1.0        document\n",
              "2  q1      2    d3     2    1.0        document\n",
              "3  q2      0    d1     0    3.0  first document\n",
              "4  q2      1    d2     1    1.0  first document\n",
              "5  q2      2    d3     2    1.0  first document"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldY8VV8wQ60Z"
      },
      "source": [
        "## Scaling up to Covid-19 Data\n",
        "\n",
        "Let's move on to our full dataset, CORD19, which is easily accessible online. We'll use PyTerrier's `get_dataset()` function to download this corpus automatically and then to index it.\n",
        "\n",
        "### Task 1: Indexing data (5 points)\n",
        "\n",
        "You first task will be to write three lines of code that create the index using an indexer or, if the index was already created, loads the created index from file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766,
          "referenced_widgets": [
            "e5a8240ceb63417c84032de0f7a1b283",
            "3e1e8ba53b57469ab09eadb6058a06f8",
            "14e424bc221b4c8d9efc27b6bc607e2b",
            "d2e8e046af6a47f5b2437b00d418e3b1",
            "6bcdb827cd904aa294b169734611699a",
            "9554506835ea48619d628b07b45bca73",
            "ef448bb09ad74fe691fe032bbf0e1867",
            "a69fd279ee3b45abbab6d88eb48efc53",
            "9d6a04fcdd884a93a82af8155d331727",
            "1f9ad2bf8f8b4b9eb28ad7acfcb9d70e",
            "e5eec716ed0b433ebadc843048dfcd3e"
          ]
        },
        "id": "L2lJsK-vEcQx",
        "outputId": "4cc13637-6af4-478a-d204-04bf6dafd8be"
      },
      "source": [
        "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
        "pt_index_path = './terrier_trec_covid'\n",
        "\n",
        "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
        "\n",
        "    # create the index, using the IterDictIndexer indexer \n",
        "    indexer = pt.IterDictIndexer(pt_index_path)\n",
        "\n",
        "    # we give the dataset get_corpus_iter() directly to the indexer\n",
        "    # while specifying the fields to index and the metadata to record\n",
        "    index_ref = indexer.index(cord19.get_corpus_iter(), fields=('abstract',), meta=('docno',))\n",
        "    \n",
        "else:\n",
        "    # if you already have the index, create an IndexRef from the data in pt_index_path\n",
        "    # that we can use to load using the IndexFactory\n",
        "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
        "    \n",
        "index = pt.IndexFactory.of(index_ref)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] If you have a local copy of https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv, you can symlink it here to avoid downloading it again: /root/.ir_datasets/downloads/80d664e496b8b7e50a39c6f6bb92e0ef\n",
            "[INFO] [starting] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv\n",
            "[INFO] [finished] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: [00:06] [269MB] [39.8MB/s]\n",
            "[INFO] [starting] building docstore\n",
            "docs_iter: 100%|████████████████████| 192509/192509 [00:14<00:00, 13177.69doc/s]\n",
            "[INFO] [finished] docs_iter: [00:14] [192509doc] [13176.21doc/s]\n",
            "[INFO] [finished] building docstore [14.62s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5a8240ceb63417c84032de0f7a1b283",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "cord19/trec-covid documents:   0%|          | 0/192509 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17:24:54.874 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (6iu1dtyl) - further warnings are suppressed\n",
            "17:26:27.669 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 54937 empty documents\n",
            "17:26:27.796 [ForkJoinPool-1-worker-3] ERROR org.terrier.structures.indexing.Indexer - Could not finish MetaIndexBuilder: \n",
            "java.io.IOException: Key 8lqzfj2e is not unique: 37597,11755\n",
            "For MetaIndex, to suppress, set metaindex.compressed.reverse.allow.duplicates=true\n",
            "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.mergeTwo(FSOrderedMapFile.java:1374)\n",
            "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.close(FSOrderedMapFile.java:1308)\n",
            "\tat org.terrier.structures.indexing.BaseMetaIndexBuilder.close(BaseMetaIndexBuilder.java:321)\n",
            "\tat org.terrier.structures.indexing.classical.BasicIndexer.createDirectIndex(BasicIndexer.java:346)\n",
            "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:369)\n",
            "\tat org.terrier.python.ParallelIndexer$1.apply(ParallelIndexer.java:63)\n",
            "\tat org.terrier.python.ParallelIndexer$1.apply(ParallelIndexer.java:52)\n",
            "\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)\n",
            "\tat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n",
            "\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\n",
            "\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\n",
            "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952)\n",
            "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926)\n",
            "\tat java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327)\n",
            "\tat java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736)\n",
            "\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919)\n",
            "\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\n",
            "\tat java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558)\n",
            "\tat org.terrier.python.ParallelIndexer$3.call(ParallelIndexer.java:133)\n",
            "\tat org.terrier.python.ParallelIndexer$3.call(ParallelIndexer.java:130)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448)\n",
            "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
            "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
            "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7GK9uANRt8w"
      },
      "source": [
        "### Task 2: 3 points\n",
        "- Print out the statistics of the index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNAVqf9uRr2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac12b27-da05-4570-81ed-f640d57b2300"
      },
      "source": [
        "print(index.getCollectionStatistics().toString())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 192509\n",
            "Number of terms: 151235\n",
            "Number of postings: 11554033\n",
            "Number of fields: 1\n",
            "Number of tokens: 17728468\n",
            "Field names: [abstract]\n",
            "Positions:   false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQD9Q8CqSirN"
      },
      "source": [
        "As a curated collection, CORD19 has a corresponding set of queries, referred to as _topics_, and the relevance assessments for each query (i.e., topic), referred to as _qrels_. We use these to evaluate as a *test collection*. PyTerrier allows us to easily access the topics (queries) and qrels from the dataset. Like much of the inputs and outputs, these are expressed as dataframes as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "8n7oY-YYS_-A",
        "outputId": "1c8b5e3b-cba4-45d8-c805-9a0b90cab833"
      },
      "source": [
        "cord19.get_topics(variant='title').head(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] [starting] https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml\n",
            "[INFO] [finished] https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml: [00:00] [18.7kB] [5.40MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>coronavirus origin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>coronavirus response to weather changes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>coronavirus immunity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>how do people die from the coronavirus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>animal models of covid 19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid                                    query\n",
              "0   1                       coronavirus origin\n",
              "1   2  coronavirus response to weather changes\n",
              "2   3                     coronavirus immunity\n",
              "3   4   how do people die from the coronavirus\n",
              "4   5                animal models of covid 19"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "-rYxqvhJTGNX",
        "outputId": "13b6cf72-1541-49cc-badf-fc356e5da906"
      },
      "source": [
        "cord19.get_qrels().head(5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] [starting] https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt\n",
            "[INFO] [finished] https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt: [00:00] [1.14MB] [6.55MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docno</th>\n",
              "      <th>label</th>\n",
              "      <th>iteration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>005b2j4b</td>\n",
              "      <td>2</td>\n",
              "      <td>4.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>00fmeepz</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>010vptx3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0194oljo</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>021q9884</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  qid     docno  label iteration\n",
              "0   1  005b2j4b      2       4.5\n",
              "1   1  00fmeepz      1         4\n",
              "2   1  010vptx3      2       0.5\n",
              "3   1  0194oljo      1       2.5\n",
              "4   1  021q9884      1         4"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gop4-jVbIIu"
      },
      "source": [
        "### Weighting Models\n",
        "\n",
        "In the earlier example, we used the simple \"`Tf`\" as our ranking function for document retrieval in BatchRetrieve. However, we can use other models such as `\"TF_IDF\"` by simply changing the `wmodel=\"Tf\"` keyword argument in the constructor of `BatchRetrieve`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Cg8AGzCibdPG",
        "outputId": "191281e9-7fc4-42d3-bb13-b3023a8cf782"
      },
      "source": [
        "tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
        "tfidf.search(\"chemical reactions\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "      <th>docno</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>18717</td>\n",
              "      <td>iavwkdpr</td>\n",
              "      <td>0</td>\n",
              "      <td>11.035982</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>171636</td>\n",
              "      <td>v3blnh02</td>\n",
              "      <td>1</td>\n",
              "      <td>10.329726</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>147193</td>\n",
              "      <td>ei4rb8fr</td>\n",
              "      <td>2</td>\n",
              "      <td>10.317138</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>121217</td>\n",
              "      <td>msdycum2</td>\n",
              "      <td>3</td>\n",
              "      <td>9.653734</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>170863</td>\n",
              "      <td>sj8i9ss2</td>\n",
              "      <td>4</td>\n",
              "      <td>9.500211</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1</td>\n",
              "      <td>2428</td>\n",
              "      <td>38aabxh1</td>\n",
              "      <td>995</td>\n",
              "      <td>3.790183</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1</td>\n",
              "      <td>14752</td>\n",
              "      <td>u709r8ss</td>\n",
              "      <td>996</td>\n",
              "      <td>3.790183</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1</td>\n",
              "      <td>20074</td>\n",
              "      <td>wxi1xsbo</td>\n",
              "      <td>997</td>\n",
              "      <td>3.790183</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1</td>\n",
              "      <td>117156</td>\n",
              "      <td>ts3obwts</td>\n",
              "      <td>998</td>\n",
              "      <td>3.790183</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1</td>\n",
              "      <td>149517</td>\n",
              "      <td>6i3x49p8</td>\n",
              "      <td>999</td>\n",
              "      <td>3.790183</td>\n",
              "      <td>chemical reactions</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    qid   docid     docno  rank      score               query\n",
              "0     1   18717  iavwkdpr     0  11.035982  chemical reactions\n",
              "1     1  171636  v3blnh02     1  10.329726  chemical reactions\n",
              "2     1  147193  ei4rb8fr     2  10.317138  chemical reactions\n",
              "3     1  121217  msdycum2     3   9.653734  chemical reactions\n",
              "4     1  170863  sj8i9ss2     4   9.500211  chemical reactions\n",
              "..   ..     ...       ...   ...        ...                 ...\n",
              "995   1    2428  38aabxh1   995   3.790183  chemical reactions\n",
              "996   1   14752  u709r8ss   996   3.790183  chemical reactions\n",
              "997   1   20074  wxi1xsbo   997   3.790183  chemical reactions\n",
              "998   1  117156  ts3obwts   998   3.790183  chemical reactions\n",
              "999   1  149517  6i3x49p8   999   3.790183  chemical reactions\n",
              "\n",
              "[1000 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6aZGX9sbdmc"
      },
      "source": [
        "Note that, as expected, because we switched the ranking, the scores of documents ranked by `TF_IDF` are no longer integers. You can see the exact TF-IDF formula used by Terrier from [the Github repo](https://github.com/terrier-org/terrier-core/blob/5.x/modules/core/src/main/java/org/terrier/matching/models/TF_IDF.java#L79)--sometimes helpful to know since there are multiple ways of defining TF-IDF! Terrier supports many weighting models and the documentation contains [a list of supported models](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ0j9lFfx-gO"
      },
      "source": [
        "## Evaluating and Comparing IR Models\n",
        "\n",
        "How do we know which of the models we've made so far are good IR models? PyTerrier provides a robust and extensive framework to help us automate the evaluation of IR models once we've defined them.\n",
        "\n",
        "As a first pass, let's take a look at the relevance scores in the dataset. To do this, we'll merge (`join`) the `qrels` with the results of our ranker to produce a dataframe that has both the ranking model's predictions (`\"score\"`) and the actual relevance score (`\"label\"`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "iyShZYpwwNSx",
        "outputId": "52d0cb8f-1651-4b5a-fefb-b69eeba5b450"
      },
      "source": [
        "qrels = cord19.get_qrels()\n",
        "\n",
        "def get_res_with_labels(ranker, df):\n",
        "    # get the results for the query or queries\n",
        "    results = ranker( df )\n",
        "    # left outer join with the qrels\n",
        "    with_labels = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n",
        "    return with_labels\n",
        "\n",
        "# lets get the Tf results for the first query\n",
        "get_res_with_labels(tfidf, cord19.get_topics(variant='title').head(1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>docid</th>\n",
              "      <th>docno</th>\n",
              "      <th>rank</th>\n",
              "      <th>score</th>\n",
              "      <th>query</th>\n",
              "      <th>label</th>\n",
              "      <th>iteration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>175892</td>\n",
              "      <td>zy8qjaai</td>\n",
              "      <td>0</td>\n",
              "      <td>7.080599</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>82224</td>\n",
              "      <td>8ccl9aui</td>\n",
              "      <td>1</td>\n",
              "      <td>6.775667</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>135326</td>\n",
              "      <td>ne5r4d4b</td>\n",
              "      <td>2</td>\n",
              "      <td>6.683114</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>122804</td>\n",
              "      <td>75773gwg</td>\n",
              "      <td>3</td>\n",
              "      <td>6.590340</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>122805</td>\n",
              "      <td>kn2z7lho</td>\n",
              "      <td>4</td>\n",
              "      <td>6.590340</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1</td>\n",
              "      <td>180809</td>\n",
              "      <td>0y0hau9l</td>\n",
              "      <td>995</td>\n",
              "      <td>4.214228</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1</td>\n",
              "      <td>148967</td>\n",
              "      <td>f8vbflx6</td>\n",
              "      <td>996</td>\n",
              "      <td>4.212887</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1</td>\n",
              "      <td>183189</td>\n",
              "      <td>uadfehr6</td>\n",
              "      <td>997</td>\n",
              "      <td>4.210201</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1</td>\n",
              "      <td>67321</td>\n",
              "      <td>n5hnx2c3</td>\n",
              "      <td>998</td>\n",
              "      <td>4.202319</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1</td>\n",
              "      <td>98706</td>\n",
              "      <td>ad6ztoba</td>\n",
              "      <td>999</td>\n",
              "      <td>4.202319</td>\n",
              "      <td>coronavirus origin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    qid   docid     docno  rank     score               query  label iteration\n",
              "0     1  175892  zy8qjaai     0  7.080599  coronavirus origin    1.0         1\n",
              "1     1   82224  8ccl9aui     1  6.775667  coronavirus origin    2.0         1\n",
              "2     1  135326  ne5r4d4b     2  6.683114  coronavirus origin    0.0       1.5\n",
              "3     1  122804  75773gwg     3  6.590340  coronavirus origin    2.0         5\n",
              "4     1  122805  kn2z7lho     4  6.590340  coronavirus origin    2.0         3\n",
              "..   ..     ...       ...   ...       ...                 ...    ...       ...\n",
              "995   1  180809  0y0hau9l   995  4.214228  coronavirus origin    0.0         0\n",
              "996   1  148967  f8vbflx6   996  4.212887  coronavirus origin    0.0         0\n",
              "997   1  183189  uadfehr6   997  4.210201  coronavirus origin    2.0       1.5\n",
              "998   1   67321  n5hnx2c3   998  4.202319  coronavirus origin    0.0         0\n",
              "999   1   98706  ad6ztoba   999  4.202319  coronavirus origin    0.0         0\n",
              "\n",
              "[1000 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngeNlZpxJe4s"
      },
      "source": [
        "### Running an Experiment\n",
        "\n",
        "We don't actually need to produce that dataframe to do our evaluation though! PyTerrier lets us run different results with an [Experiment](https://pyterrier.readthedocs.io/en/latest/experiments.html) object, which will compare models according to the evaluation metrics we specify. Here, let's run an experiment to evaluate our `tfidf` model that we created earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "OFUmiFSobUDg",
        "outputId": "074ddcd9-89fe-4740-e2a1-2e3211d92321"
      },
      "source": [
        "pt.Experiment(\n",
        "    [tfidf],\n",
        "    cord19.get_topics(variant='title'),\n",
        "    cord19.get_qrels(),\n",
        "    eval_metrics=[\"map\", \"ndcg\"])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>map</th>\n",
              "      <th>ndcg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BR(TF_IDF)</td>\n",
              "      <td>0.180002</td>\n",
              "      <td>0.370767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         name       map      ndcg\n",
              "0  BR(TF_IDF)  0.180002  0.370767"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt0iPhRw2J-S"
      },
      "source": [
        "## Task 3: Define new models and evaluate them in an Experiment (28 points)\n",
        "\n",
        "Now comes the fun part! Your task is to define **three** new [`BatchRetrieve`](https://pyterrier.readthedocs.io/en/latest/terrier-retrieval.html#batchretrieve) objects with different word ranking methods. You are welcome to set the hyperparameters but all models should be sufficiently different. You are definitely welcome (encouraged, even!) to compare _more_ than three models too.\n",
        "\n",
        "Once you have defined your three `BatchRetrieve` objects, conduct an `Experiment` using all of them _at once_ (not three separate `Experiment` runs!) to evaluate the results.  Your experiment should include the two metrics used above, as well as NDCG for the top-5 and top-10 results. You are welcome to include other metrics as well\n",
        "\n",
        "Print the results of the Experiment and then write 2-3 sentences (or more) about what you see in the performance. Is there a clear better model? Would you expect better performance with some hyperparameter tuning?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRjyEZ5_aTvM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7b248707-e5a2-4bd0-bb76-08d18e027387"
      },
      "source": [
        "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
        "dph = pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
        "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
        "hiemstra_lm = pt.BatchRetrieve(index, wmodel=\"Hiemstra_LM\")\n",
        "\n",
        "pt.Experiment(\n",
        "    [bm25, dph, pl2, hiemstra_lm],\n",
        "    cord19.get_topics(variant='title'),\n",
        "    cord19.get_qrels(),\n",
        "    eval_metrics=[\"map\", \"recip_rank\", \"ndcg\", \"ndcg_cut_5\", \"ndcg_cut_10\", \"P_5\", \"P_10\"])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>map</th>\n",
              "      <th>recip_rank</th>\n",
              "      <th>ndcg</th>\n",
              "      <th>ndcg_cut_5</th>\n",
              "      <th>ndcg_cut_10</th>\n",
              "      <th>P_5</th>\n",
              "      <th>P_10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BR(BM25)</td>\n",
              "      <td>0.181478</td>\n",
              "      <td>0.808759</td>\n",
              "      <td>0.373328</td>\n",
              "      <td>0.611724</td>\n",
              "      <td>0.583665</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BR(DPH)</td>\n",
              "      <td>0.177567</td>\n",
              "      <td>0.767259</td>\n",
              "      <td>0.367698</td>\n",
              "      <td>0.603074</td>\n",
              "      <td>0.584309</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BR(PL2)</td>\n",
              "      <td>0.169525</td>\n",
              "      <td>0.785992</td>\n",
              "      <td>0.358597</td>\n",
              "      <td>0.583850</td>\n",
              "      <td>0.554204</td>\n",
              "      <td>0.668</td>\n",
              "      <td>0.620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BR(Hiemstra_LM)</td>\n",
              "      <td>0.151881</td>\n",
              "      <td>0.623883</td>\n",
              "      <td>0.342010</td>\n",
              "      <td>0.427316</td>\n",
              "      <td>0.421561</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.482</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              name       map  recip_rank  ...  ndcg_cut_10    P_5   P_10\n",
              "0         BR(BM25)  0.181478    0.808759  ...     0.583665  0.700  0.652\n",
              "1          BR(DPH)  0.177567    0.767259  ...     0.584309  0.684  0.656\n",
              "2          BR(PL2)  0.169525    0.785992  ...     0.554204  0.668  0.620\n",
              "3  BR(Hiemstra_LM)  0.151881    0.623883  ...     0.421561  0.500  0.482\n",
              "\n",
              "[4 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Xfce5NJe4t"
      },
      "source": [
        "It seems that the performance of several models is relatively similar, and it is difficult to find an absolutely best one.<br/>\n",
        "The BM25 model performs better in the overall performance and in the first 5 retrieved documents. However, the DPH model performs a little better than BM25 in the first 10 retrieved documents.<br/>\n",
        "With some hyperparameter tuning, the performance of the models may be a little better."
      ]
    }
  ]
}